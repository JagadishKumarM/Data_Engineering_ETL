## From Web Scraping to Cloud Deployment: End-to-End Data Pipeline Automation with MySQL, Python, and AWS

This project showcases the end-to-end process of building an **ETL data pipeline** that collects and processes city information and weather data. 
By leveraging web scraping techniques, a relational database, and cloud services, the pipeline is made robust and scalable. 
This experience highlights my skills in data engineering, from writing efficient scraping scripts to managing databases and deploying cloud-based solutions.

The primary objectives of this project are:

-**Data Extraction**: Scrape city information from Wikipedia and weather data (5 day forecast) from OpenWeatherMap.

-**Data Transformation and Cleaning**: Clean the extracted data to ensure consistency and accuracy.

-**Data Loading**: Store the transformed data in a MySQL database.( Local Machine and AWS RDS Cloud ) 

-**Pipeline Deployment**: Deploy the ETL pipeline using AWS services.

-**Pipeline Automation**: Automate the data collection and storage process with AWS Lambda,RDS and S3.

Check out my detailed article on this project:
**https://medium.com/@mjkjagadishkumarofficial/web-scraping-to-cloud-deployment-end-to-end-data-engineering-pipeline-automation-with-mysql-0e1534fa2a4c**

This project not only strengthened my understanding of data engineering but also enhanced my ability to work with various tools and technologies essential for building scalable data solutions. 
Please feel free to connect with me, if you have any questions or need further information.
Thank you!
